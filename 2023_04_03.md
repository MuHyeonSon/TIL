- “Attention is all you need” 논문 리뷰
    - RNN 계열 모델들은 어떠한 기법을 적용하여 개선한다고 하더라도 순차적 계산을 한다는 것은 변함 없고 순차적 계산을 할 때 입력 sequence가 길어질 수록 연산량 증가와 학습 속도 느려짐 등의 문제를 해결할 수 없다는 것을 알게 되었음.
    - Transformer는 병렬연산을 통해 입력 sequence를 한 번에 처리할 수 있다는 것을 알게 됨.
    - 기존에도 Attention 개념이 있었지만, RNN과 함께 사용했어야 돼서 RNN의 문제점을 해결하지 못해서 Recurrent나 convolution을 아예 사용하지 않고 오로지 Attention만을 가지고 sequence 변환 모델을 만들어 보자고해서 만든 것이 Transformer라는 것을 알게 됨.
    - 논문에서 제안하는 Attention에는 크게 2가지 종류의 Attention의 종류가 존재한 다는 것을 알게 됨. (3가지로 분류하는 사람도 있음)
        - Scaled Dot-Product Attention
        - Multi-Head Attention
        - Relative Positional Encoding
- skip connection은 출력과 입력의 차이를 학습한다는 말이 아직도 이해가 되지 않았음.
