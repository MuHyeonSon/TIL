- 실습 1-1, 1-2를 진행하며, Naive Bayise classification을 직접 구현해보며, 내부적으로 likliehood, 사전확률, 라플라스 스무딩 등에 대해 공부하며 코드로 직접 구현해보고 주석을 달아보았음
- word2vec 이 무엇인지, 학습은 어떻게 되는지 등에 대해 학습하고 정리하였음.
- 부캠살롱을 통해 NLP 기반 작곡프로그램이 어떤 것이 있는지 알아보았고, OpenAI의 쥬크박스라는 것이 있는 것을 알게 되었음.
- re모듈을 사용해보며 정규표현식에 대해 학습하였다.
- 멘토링 시간을 통해 BERT의 구조와 pytorch에서 구현된 클래스를 내부적으로 뜯어보며 살펴보았다. 또한 Hugging face에 대한 설명을 통해 사용하면 좋을 모델들에 대해 알게 되었다.
    - “##”은 왜 붙는지 ⇒ “play” “##ing” 두 단어가 tokenizing 하기 전에 원래는 한 단어였다는 것을 알려주는 표시
    - CLS를 넣으면 성능이 좋다는 것은 경험적 연구결과이며 관용적으로 사용되고 있음
    - MLM : 나는 강아지 좋다 (단어 대 단어)
    - NSP : 나는 ~ , 나는~ (문장  대 문장) ??
    - ResNet의 Residual 구조가 맞음, N번 반복은 서로 다른 weight를 가진 구조가 여러개가 있다는 것.
        
        ![Untitled](https://s3-us-west-2.amazonaws.com/secure.notion-static.com/10280927-12f1-45f7-8155-4e211a2c5b6e/Untitled.png)
        
    - Bert(Classification)의 결과는 마지막 output차원에 따라 3가지 경우가 있음
        1. num_labels = 3  (W sigmoid) ⇒ [0.2, 0.3, 0,5]
            1. 감정분류task 에서 [긍정, 중립, 부정]
        2. num_labels = 1 
        3. num_label = 1
            1. 2,3,의 경우 유사도 측정
                1. 문장에 대해 점수를 구한다.
                2. 문장과 문장간의 유사도
                3. 문장 - 얼마나 자연스러운가
                4. 에세이 자동채점
                5. report 자동채점
    - chatGPT는 강화학습으로 탄생함
        - Human feedback
