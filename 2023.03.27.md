- Sequence to Sequence Learning with Neural Networks 논문을 읽으며, 기존의 SMT방법이 연산량이 많이 들어 이를 논문에서 제시하는  4층 딥 layer LSTM 모델이 보완할 수 있다는 것을 알게 되었음.  DNN은 입력과 출력길이가 가변적인 문제를 풀 수 없기 때문에 RNN이 등장했지만 RNN은 Long term dependency 문제가 있어, 이를 해결하기 위해 RNN 대신 LSTM을 사용하면 어느 정도 문제가 해결 된다는 것을 알 수 있었음.
- 또한 입력 단어를 역순으로 넣어 학습시켰을 때, 긴 문장에 대해서도 성능이 잘 나온다는 것을 발견했고, 이는 short term dependency를 많이 도입하여 최적화를 진행할 수 있기 때문이라고 했다.
- 또한 다른 사람 발표를 들으며, 알게된 내용들이 있는데,  해당 논문의 연구자들이 실험을 해보고 잘 나와서 자신들이 먼저 논문을 게재시키기 위해 충분히 실험을 하지는 못했지만, 그럼에도 성능이 잘 나왔다는 것이다.
- 논문을 읽으며, 내가 해당 논문의 모**델 학습 방법에 제대로 잘 이해하지 못하고 있다는 것**을 알게되었고,
- **실제 문장들에 대해서 LSTM의 입출력이 어떻게 이루어지는 제대로 알지 못하고 있다**는 것을 알게 되었다.
