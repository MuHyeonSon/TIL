- 어제 CNN을 공부하면서 LesNet의 skip connection을 통해 입력과 출력의 차이를 학습 한다는 것이 이해가 안갔고,
- → x 가 들어갔을 때 f(x)가 나오도록 학습해야됨 원래는 즉, x가 어떤 출력값이 나오도록 학습을 하는 건데, skip connection의 경우는  x가 x 자신과 출력y의 차이가 되도록 학습한다
- **backpropagation에 대한 이해**가 부족한다는 것을 알게되었다.
    - MLP에서 Loss 계산할 때, 무엇에 대해 미분하냐(x? y? w? b?)
    - backpropagation 과정내의 그라디언트를 구하는 거 아닌가?
    - 그렇다면 그라디언트 값을 전달하는 건 update과정에서 일어나는 거 아닌가?
    - backpropagation ( loss에 대한 그라디언트를 구한다 → 구한 그래디언트를 사용하여(optimizer)가 parameter를 업데이트한다)
