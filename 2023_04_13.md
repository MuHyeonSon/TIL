- 질문(BERT와 CLS토큰)
    
    ### BERT 모델과 CLS 토큰
    
    BERT (Bidirectional Encoder Representations from Transformers) 모델에서 CLS (Classification) 토큰은 다음과 같은 이유로 사용됩니다.
    
    1. 분류 작업을 수행하기 위해
    CLS 토큰은 BERT 모델의 입력 시퀀스의 첫 번째 토큰으로 사용됩니다. 이는 분류 작업을 수행할 때 출력값을 결정하는 데 사용됩니다. 예를 들어, 텍스트 분류 작업에서는 입력 텍스트가 어떤 클래스에 속하는지 예측하는 것이 목표입니다. 이때, BERT 모델은 CLS 토큰의 임베딩 벡터를 분류 작업을 위한 입력으로 사용합니다.
    2. Fine-tuning을 위한 정보 제공
    CLS 토큰은 모델이 fine-tuning을 수행할 때 추가적인 정보를 제공합니다. fine-tuning 단계에서는 미리 학습된 BERT 모델의 가중치를 업데이트하여 새로운 작업에 맞게 조정하는 작업입니다. CLS 토큰의 임베딩 벡터는 fine-tuning 과정에서 사용되는 마지막 출력 벡터로 사용됩니다. 따라서, CLS 토큰은 fine-tuning 과정에서 핵심적인 역할을 합니다.
    3. 모델 간 분류 성능 비교를 위해
    CLS 토큰은 BERT 모델을 비롯한 다른 transformer 모델들 간의 분류 성능 비교를 위해 사용됩니다. CLS 토큰은 모든 입력 시퀀스에서 동일한 위치에 있으므로, 이를 사용하여 각 모델의 분류 성능을 쉽게 비교할 수 있습니다.
    
    따라서, BERT 모델에서 CLS 토큰은 분류 작업의 결과를 출력하기 위한 입력으로 사용되며, fine-tuning 과정에서도 중요한 역할을 합니다. 또한, 모델 간 분류 성능을 비교하는 데에도 유용합니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### CLS 토큰의 임베딩 벡터는 fine-tuning 과정에서 사용되는 마지막 출력 벡터로 사용된다는 것
    
    Fine-tuning 과정에서는 미리 학습된 BERT 모델의 가중치를 새로운 작업에 맞게 조정하는 작업을 수행합니다. 이때, fine-tuning 데이터셋을 이용해 모델을 학습하고, 학습된 모델은 입력 문장을 분류하는 작업에서 사용됩니다.
    
    Fine-tuning 과정에서 BERT 모델은 입력 시퀀스의 모든 토큰에 대한 출력값을 계산하며, 이를 통해 입력 문장을 분류하는 데 사용됩니다. 이때, 출력값 중 첫 번째 토큰(CLASS)에 해당하는 임베딩 벡터는 fine-tuning 작업의 결과를 출력하기 위해 사용됩니다.
    
    즉, fine-tuning 과정에서 BERT 모델은 입력 시퀀스의 모든 토큰에 대한 출력값을 계산하며, 이 중에서도 CLS 토큰의 임베딩 벡터를 출력값으로 사용하여 분류 작업을 수행합니다. 따라서, CLS 토큰은 fine-tuning 작업에서 모델의 핵심적인 역할을 합니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### CLS 토큰이 모델의 입력 전과 출력으로 나오는 형태가 어떻게 되는 지
    
    예를 들어, BERT 모델을 사용하여 영화 리뷰를 긍정적인지 부정적인지 분류하는 작업을 수행한다고 가정해보겠습니다. 이때, 입력 문장이 "이 영화 정말 재미있었어!"인 경우, BERT 모델의 입력 시퀀스는 다음과 같습니다.
    
    [CLS] 이 영화 정말 재미있었어! [SEP]
    
    여기서 [CLS]는 문장의 시작을 나타내는 토큰입니다. [SEP]은 BERT 모델에서 문장 사이를 구분하는 역할을 하는 토큰입니다.
    
    BERT 모델은 입력 시퀀스를 임베딩하여 출력값을 계산하고, 이를 분류 작업에 사용합니다. 이때, 분류 작업의 결과는 CLS 토큰의 임베딩 벡터를 출력값으로 사용합니다.
    
    따라서, BERT 모델의 출력값은 다음과 같습니다.
    
    [CLS] -> [0.1, -0.3, 0.5, ...]
    
    이때, 임베딩 벡터의 크기는 모델의 설정에 따라 달라질 수 있으며, 위 예시에서는 임베딩 벡터의 크기가 768인 경우를 가정하였습니다.
    
    이 출력값은 모델이 예측한 긍정적인 또는 부정적인 확률값으로, 분류 작업에서 사용됩니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 그럼 [CLS] -> [0.1, -0.3, 0.5, ...]에서  [CLS]는 무슨 역할을 하는 건가요
    
    [CLS] 토큰은 문장의 시작을 나타내는 특별한 토큰입니다. BERT 모델은 입력 문장의 시작을 나타내는 [CLS] 토큰을 특별하게 처리하여 해당 문장이 어떤 의미를 가지는지를 학습합니다.
    
    [CLS] 토큰은 BERT 모델에서 입력 시퀀스의 처음에 위치하며, 이를 통해 모델이 문장의 시작 위치를 파악할 수 있습니다. 이를 통해 모델은 문장의 의미를 파악하고, 해당 문장에 대한 분류 작업을 수행할 수 있습니다.
    
    또한, [CLS] 토큰의 임베딩 벡터는 fine-tuning 과정에서 출력값으로 사용됩니다. 이때, 분류 작업의 결과는 [CLS] 토큰의 임베딩 벡터를 출력값으로 사용하여 수행됩니다. 따라서, [CLS] 토큰은 BERT 모델에서 매우 중요한 역할을 수행하는 토큰입니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### [CLS] 토큰이 분류작업에서 출력으로 확률분포형태로 나옴.
    
    맞습니다. [CLS] 토큰은 분류 작업에서 모델의 출력값으로 사용됩니다. BERT 모델은 [CLS] 토큰의 임베딩 벡터를 추출하여, 이를 분류 작업의 입력으로 사용합니다. 이때, 분류 작업의 출력값은 보통 확률 분포의 형태로 나타내어집니다.
    
    예를 들어, 이진 분류 작업에서는 긍정과 부정의 두 가지 클래스 중 하나를 선택하는 문제입니다. BERT 모델은 [CLS] 토큰의 임베딩 벡터를 입력으로 받아, 해당 문장이 긍정적인지 혹은 부정적인지를 예측하는 확률값을 출력합니다. 이때, 출력값은 보통 sigmoid 함수를 사용하여 [0,1] 범위의 확률값으로 나타내어집니다.
    
    다중 분류 작업에서는 여러 개의 클래스 중 하나를 선택하는 문제입니다. 예를 들어, 영화 리뷰의 경우 긍정, 부정, 중립 등의 클래스로 분류할 수 있습니다. BERT 모델은 [CLS] 토큰의 임베딩 벡터를 입력으로 받아, 해당 문장이 각 클래스에 속할 확률값을 출력합니다. 이때, 출력값은 보통 softmax 함수를 사용하여 각 클래스별 확률값으로 나타내어집니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 그럼 분류 문제가 아닌 경우에 CLS 토큰이 사용되는지
    
    네, 분류 문제가 아닌 경우에도 [CLS] 토큰은 사용될 수 있습니다.
    
    예를 들어, 문장 유사도를 측정하는 태스크에서는 두 문장의 [CLS] 임베딩 벡터 간의 코사인 유사도를 계산하여 문장 간의 유사도를 측정할 수 있습니다.
    
    또한, BERT 모델은 다른 태스크에도 적용 가능한 범용 언어 모델로서 활용될 수 있습니다. 이 경우, [CLS] 토큰은 태스크에 따라 다른 용도로 활용될 수 있습니다. 예를 들어, BERT를 이용한 문장 생성 태스크에서는 [CLS] 토큰의 임베딩 벡터를 초기값으로 사용할 수 있습니다. 이러한 방법을 통해, 모델이 생성하는 문장의 품질을 개선할 수 있습니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### BERT모델의 임베딩 차원
    
    BERT 모델의 임베딩 차원은 모델의 사전학습(pre-training) 단계에서 결정되며, 모델의 크기에 따라 다릅니다.
    
    일반적으로 BERT 모델에서는 임베딩 차원이 768, 1024 등의 크기를 가집니다. BERT-base 모델은 768차원의 임베딩을 사용하고 있으며, BERT-large 모델은 1024차원의 임베딩을 사용하고 있습니다.
    
    이러한 차원 크기는 모델의 파라미터 수와 연관이 있으며, 더 큰 임베딩 차원은 더 많은 파라미터를 가진 모델을 의미합니다. 따라서 더 큰 임베딩 차원을 가진 모델은 더 복잡한 문제를 해결할 수 있지만, 그만큼 학습에 더 많은 계산 자원과 시간이 필요합니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 만약 임베딩 차원이 768이라면 768이라는 것은 어떤 것의 숫자인지?
    
    BERT 모델의 임베딩 차원이 768이라는 것은, 각 단어를 나타내는 벡터의 크기가 768이라는 의미입니다. 이 벡터는 단어의 의미를 수치적으로 표현한 것으로, BERT 모델의 사전학습 과정에서 학습됩니다. 이렇게 학습된 단어 벡터는 문장의 의미를 이해하고, 다양한 자연어 처리 태스크를 수행하는 데 사용됩니다.
    
    단어 벡터의 각 차원은 서로 다른 정보를 담고 있으며, 이를 통해 단어 간의 의미적, 구문적 관계를 파악하고 문장의 의미를 이해합니다. 예를 들어, "강아지"와 "고양이"라는 두 단어는 비슷한 개념을 나타내기 때문에, 이들의 벡터 표현도 서로 비슷한 패턴을 가지게 됩니다.
    
    따라서, BERT 모델의 임베딩 차원이 768이라는 것은, 모델이 자연어 처리 태스크를 수행하기 위해 768개의 차원으로 이루어진 벡터를 사용한다는 것을 의미합니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    그러면 "강아지와 고양이"라는 문장을 임베딩하면 벡터의  형태는 어떻게 되는지
    
    "강아지와 고양이"라는 문장을 BERT 모델의 임베딩으로 변환하면, 다음과 같은 형태의 벡터로 표현됩니다:
    
    ```
    cssCopy code
    [-0.0231, 0.7892, -0.3247, ..., -0.7361, 0.1228, 0.2520]
    
    ```
    
    이 벡터는 768개의 요소로 이루어져 있으며, 각 요소는 문장에서 하나의 단어를 나타냅니다. 이 경우 "강아지"와 "고양이"가 각각 하나의 단어이므로, 이 두 단어를 나타내는 요소가 각각 존재합니다.
    
    이 벡터는 "강아지와 고양이"라는 문장의 의미를 담고 있으며, 이 벡터를 이용하여 다양한 자연어 처리 태스크를 수행할 수 있습니다. 예를 들어, 이 벡터를 입력으로 받아 문장의 감성 분류, 질문 응답, 문장 유사도 측정 등의 태스크를 수행할 수 있습니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    
  토큰화된 문장이 입력으로 들어오면, BERT 모델은 각 토큰을 벡터로 임베딩하여 문장 전체를 하나의 벡터로 변환합니다.
    
    따라서, "강아지와 고양이"라는 문장을 BERT 모델에 입력으로 넣기 전에는 토큰화 과정을 거쳐야 하며, 이 과정에서 각 단어는 하나의 토큰으로 분리됩니다. 각 토큰에 대한 임베딩 벡터가 계산되어 최종적으로 "강아지", "와", "고양이" 세 개의 벡터가 결합된 하나의 벡터가 만들어집니다.
    
    따라서, 정확하게 말씀드리자면, "강아지와 고양이"라는 문장을 BERT 모델에 입력으로 넣으면, 이 문장은 토큰화되어 각 토큰의 임베딩 벡터가 계산된 후, 이들 벡터가 결합되어 하나의 문장 임베딩 벡터가 만들어지게 됩니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    
    
     "강아지와 고양이"라는 문장을 BERT 모델에 입력으로 넣었을 때, 토큰화된 결과는 다음과 같이 됩니다.
    
    ```
    cssCopy code
    [CLS] 강아지 와 고양이 [SEP]
    
    ```
    
    여기에서 **`[CLS]`**와 **`[SEP]`**은 BERT 모델에서 사용하는 특수 토큰입니다. **`[CLS]`** 토큰의 역할은 앞서 설명드렸던 것처럼, 전체 문장을 대표하는 벡터를 만들기 위한 시작점 역할을 합니다. **`[SEP]`** 토큰은 서로 다른 두 문장을 구분하기 위한 구분자 역할을 합니다.
    
    따라서, 위의 토큰화된 결과를 BERT 모델에 입력으로 넣으면, 각 토큰의 임베딩 벡터가 계산되어 최종적으로 "강아지", "와", "고양이" 세 개의 벡터가 결합된 하나의 문장 임베딩 벡터가 만들어집니다. 이 문장 임베딩 벡터의 차원은 BERT 모델의 하이퍼파라미터 중 하나인 **`hidden_size`**에 해당하는 값, 즉 일반적으로는 768입니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 그럼 각 토큰들은 사전에서 그 토큰에 해당하는 id로 맵핑되는 건지
    
    네, 각 토큰은 BERT 모델이 사용하는 사전에서 그 토큰에 해당하는 고유한 ID 값으로 맵핑됩니다. 이를 우리는 보통 "어휘집(vocabulary)" 또는 "토큰 임베딩 테이블(token embedding table)"이라고 부릅니다. BERT 모델에서는 토큰들을 서로 다른 유형의 토큰으로 분류하고, 각 토큰 유형에 따라 해당 토큰들을 처리하는 방식이 다릅니다. 예를 들어, **`[CLS]`** 토큰, **`[SEP]`** 토큰, 문장 내 단어 토큰 등 각각의 토큰 유형마다 임베딩 테이블 내의 위치와 처리 방식이 다릅니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 그러면 "강아지와 고양이"라는 문장이 토큰화되고 임베딩 되어 BERT의 최종 입력이 되는 형태까지의 과정은?
    
    먼저, "강아지와 고양이"라는 문장은 토큰화(tokenization) 과정을 거쳐 BERT의 입력으로 들어갑니다. 이 때, 예를 들어 WordPiece 기반의 토큰화 방법을 사용한다면 "강아지와"는 "강아지"와 "##와"라는 두 개의 서브워드(subword) 토큰으로 분리될 수 있습니다. "고양이"도 동일한 방식으로 "고양"과 "##이"로 분리될 수 있습니다. 그러면 이제 각 토큰들은 BERT 모델의 어휘집(vocabulary)에서 각각의 고유한 ID 값으로 맵핑됩니다.
    
    예를 들어, "강아지" 토큰이 어휘집에서 ID 값 500에 해당한다고 가정하면, "##와" 토큰은 ID 값 1200에 해당하고, "고양" 토큰은 ID 값 800에 해당하고, "##이" 토큰은 ID 값 900에 해당할 수 있습니다.
    
    이렇게 맵핑된 각각의 토큰 ID 값은 BERT 모델의 입력으로 사용되며, 이들은 임베딩 테이블(token embedding table)에서 해당하는 토큰 ID 값의 임베딩 벡터로 대응됩니다. 즉, 각각의 토큰 ID 값은 해당하는 임베딩 벡터로 대체되어, "강아지와 고양이" 문장 전체는 각 토큰의 임베딩 벡터가 모인 행렬로 표현됩니다. 이 때, BERT 모델은 입력 토큰들의 임베딩 벡터를 다층 신경망 구조인 Transformer에 넣어 연산을 수행하게 됩니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 그럼 "강아지와 고양이" 문장이 임베딩벡터가 된 것은 어떤 형태인지?
    
    "강아지와 고양이" 문장이 BERT 모델에 입력되고 임베딩 된 후에는 [CLS] 토큰의 임베딩 벡터와 "강아지"와 "와", "고양이"의 임베딩 벡터가 순서대로 이어져서 하나의 벡터로 표현됩니다. 즉, [CLS] 토큰의 임베딩 벡터와 문장 내 단어들의 임베딩 벡터가 이어진 형태의 하나의 벡터가 문장 "강아지와 고양이"의 임베딩 벡터가 됩니다. 이 벡터의 차원은 BERT 모델의 임베딩 차원과 같으며, 일반적으로는 768차원입니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 각 임베딩들을 concatenate 시킨 형태가 768차원이라는 건지?
    
    네, 맞습니다. 각 토큰의 임베딩 벡터를 이어붙인(concatenate) 벡터는 BERT 모델의 임베딩 차원과 같은 차원을 가지게 됩니다. 일반적으로는 768차원이지만, BERT-base 모델의 경우 768차원이 아닌 1024차원을 가집니다. 이렇게 이어붙인(concatenate) 벡터가 문장의 임베딩 벡터가 됩니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    ### 벡터들을 옆으로 붙이면  768보다 더 길어질 수 있는 것이 아닌지
    
    네, 맞습니다. 각 토큰의 임베딩 벡터를 이어붙인(concatenate) 벡터의 길이는 토큰의 개수에 따라 달라질 수 있습니다. 예를 들어, "강아지와 고양이"라는 문장을 BERT-base 모델에 입력하면 [CLS] 토큰의 임베딩 벡터와 "강아지", "와", "고양이"의 임베딩 벡터를 이어붙인(concatenate) 벡터의 길이는 1024가 됩니다. 이 길이는 BERT-base 모델의 임베딩 차원과 같은 1024 차원입니다. 따라서, 각 임베딩을 옆으로 이어붙인(concatenate) 형태의 벡터는 임베딩 차원과 같은 길이를 가집니다.
    
    ### NLP STS task에서 [CLS] token은 어떻게 사용되는가
    
    [CLS] token은 BERT와 같은 Transformer 기반의 언어 모델에서 사용되며, 이 모델에서는 입력 시퀀스의 처음에 [CLS] token을 추가합니다. 이 token은 입력 시퀀스의 전반적인 의미를 나타내는 벡터 표현을 학습하도록 설계되었습니다.
    
    STS task에서는 입력 시퀀스의 처음에 [CLS] token을 추가하고, 이 token의 출력을 활용하여 두 문장 간의 유사도를 계산합니다. 보통은 이 token의 출력 벡터를 최종적인 분류나 회귀 모델의 입력으로 사용하거나, 유사도 점수 계산에 활용합니다.
    
    이러한 방식으로 [CLS] token은 STS task에서 매우 중요한 역할을 합니다. 그러나 [CLS] token이 항상 최상의 성능을 발휘하는 것은 아니며, 모델 아키텍처나 입력 데이터 등에 따라 다를 수 있습니다.
    
    [data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e](data:image/svg+xml,%3csvg%20xmlns=%27http://www.w3.org/2000/svg%27%20version=%271.1%27%20width=%2730%27%20height=%2730%27/%3e)
    
    그럼 max length는 무엇을 의미하나요?
    
- KLUE와 GLUE와의 차이
    
    NLP 분야에서 KLUE라는 개념은 무엇이고 GLUE와의 차이는 무엇인지
    
    KLUE와 GLUE는 모두 자연어 처리(Natural Language Processing, NLP) 분야에서 사용되는 벤치마크(benchmark) 데이터셋이며, 모델의 성능을 평가하기 위해 사용됩니다.
    
    KLUE(Korean Language Understanding Evaluation)는 한국어 자연어 이해(NLU) 작업에 대한 벤치마크 데이터셋입니다. 이 데이터셋은 7개의 다양한 NLU 작업을 포함하고 있으며, 문장 감성 분석, 질문-답변, 개체명 인식, 의도 분류 등을 포함합니다.
    
    반면에, GLUE(General Language Understanding Evaluation)는 영어를 포함한 다양한 언어의 NLU 작업에 대한 벤치마크 데이터셋입니다. 이 데이터셋은 문장 감성 분석, 질문-답변, 의미론적 유사성 등 9개의 작업을 포함하고 있습니다.
    
    따라서 KLUE는 한국어 NLU 작업에 대한 평가를 위한 데이터셋이며, GLUE는 다양한 언어의 NLU 작업에 대한 평가를 위한 데이터셋입니다. 또한, KLUE는 한국어 데이터셋으로서 GLUE와는 다른 특징을 가지고 있으며, KLUE와 GLUE는 서로 다른 평가 척도와 데이터셋 구성을 가지고 있습니다.
    
    벤치마크(benchmark)란, 일반적으로 기술 분야에서 제품이나 서비스의 성능을 측정하고 비교하는 것을 의미합니다. NLP 분야에서는 자연어 이해(NLU) 모델의 성능을 측정하고 비교하기 위한 벤치마크 데이터셋
    
    그럼 KLUE는 의미론적 유사성에 대한 데이터셋은 없는 것인지
    
    KLUE(Korean Language Understanding Evaluation) 데이터셋에는 의미론적 유사성에 대한 작업이 포함되어 있지 않습니다. KLUE 데이터셋은 한국어 자연어 이해(NLU) 작업에 대한 벤치마크 데이터셋으로, 문장 감성 분석, 질문-답변, 개체명 인식, 의도 분류 등 7개의 작업을 포함하고 있습니다.
    
    하지만 KLUE의 후속 데이터셋으로 KLUE-STS(KLUE Semantic Textual Similarity)가 추가되어 의미론적 유사성에 대한 작업도 포함되었습니다. KLUE-STS는 두 문장 사이의 의미론적 유사성 점수를 예측하는 작업으로 구성되어 있습니다. 따라서 KLUE-STS는 한국어 의미론적 유사성에 대한 평가를 위한 데이터셋으로 사용될 수 있습니다.
    
- 질문(임베딩 차원 752와 max_len = 512 면 각각 입력 임베딩 이 어떤 형태로 되는 건지
- GPU 서버 개념
    
    GPU 서버
    
    일반적으로 서버는 다수의 사용자나 프로세스가 동시에 접속하여 사용할 수 있는 컴퓨터 시스템입니다. 이러한 서버 시스템에서 GPU 서버는 그래픽 처리 장치(Graphic Processing Unit, GPU)를 포함한 하드웨어를 탑재하고 있는 시스템을 말합니다.
    
    GPU 서버는 주로 딥러닝이나 고성능 컴퓨팅 등의 분야에서 사용됩니다. 이러한 분야에서는 대용량의 데이터를 처리하거나 병렬 계산이 필요하기 때문에 GPU를 사용하여 연산 속도를 높이는 경우가 많습니다. 따라서 GPU 서버는 하드웨어 성능을 최적화하여 이러한 분야에서 고속의 연산을 수행할 수 있도록 구성되어 있습니다.
    
    또한, GPU 서버는 다수의 사용자나 프로세스가 동시에 GPU를 사용할 수 있도록 하기 위해 가상화 기술을 사용하여 가상 GPU 환경을 제공하기도 합니다. 이렇게 하면 다수의 사용자가 동시에 GPU를 사용할 수 있으며, 이를 통해 비용 효율적으로 GPU 자원을 활용할 수 있습니다.
    
- 라이브러리 조사
    - os모듈
        
        Python의 os 모듈은 운영체제(Operating System)와 상호작용하기 위한 함수를 제공하는 모듈입니다. 이 모듈을 사용하면 운영체제와 관련된 여러 작업을 수행할 수 있습니다.
        
        os 모듈은 파일 및 디렉토리 관리, 프로세스 관리, 환경 변수 설정, 시스템 명령어 실행 등의 작업을 수행할 수 있는 함수를 제공합니다. 몇 가지 예시로는 다음과 같은 함수들이 있습니다.
        
        - os.getcwd() : 현재 작업 디렉토리를 반환합니다.
        - os.listdir(path) : 해당 경로의 디렉토리 내 파일 목록을 반환합니다.
        - os.mkdir(path) : 해당 경로에 디렉토리를 생성합니다.
        - os.path.join(path, *paths) : 경로를 조합하여 새 경로를 만듭니다.
        - os.system(command) : 시스템 명령어를 실행합니다.
        
        이 외에도 os 모듈에는 다양한 함수들이 있으며, 이를 통해 운영체제와 상호작용하여 다양한 작업을 수행할 수 있습니다.
        
    - argparse
        
        Python의 argparse 모듈은 명령행 인자(command-line arguments)를 파싱하고, 인자에 대한 도움말 메시지를 자동 생성해주는 기능을 제공하는 모듈입니다. argparse 모듈은 프로그램을 실행할 때 사용자가 지정하는 인자(argument)를 파싱하여 해당 인자를 적절한 값으로 변환하고, 이를 활용하여 프로그램의 동작을 결정할 수 있도록 합니다.
        
        argparse 모듈을 사용하면, 프로그램의 동작을 사용자가 실행할 때 지정하는 인자들에 따라 쉽게 결정할 수 있습니다. 또한 argparse 모듈을 사용하면, 프로그램에 대한 설명과 사용법에 대한 도움말 메시지를 쉽게 생성할 수 있으므로, 사용자가 프로그램을 쉽게 이해하고 사용할 수 있도록 도움을 줄 수 있습니다.
        
        argparse 모듈을 사용하려면, argparse.ArgumentParser() 객체를 생성하여 해당 객체에 인자와 인자에 대한 정보를 추가하고, parse_args() 메서드를 호출하여 입력된 인자들을 파싱하면 됩니다. 이를 통해 argparse 모듈을 사용하여 사용자가 입력한 명령행 인자를 손쉽게 처리할 수 있습니다.
        
    - requests모듈
        
        Python의 requests 모듈은 HTTP 요청을 보내고 응답을 받기 위한 간편한 방법을 제공하는 모듈입니다. requests 모듈을 사용하면 HTTP 요청을 보내기 위한 복잡한 코드를 작성하지 않고도, 간단한 코드 몇 줄만으로 HTTP 요청을 보낼 수 있습니다.
        
        requests 모듈은 다양한 HTTP 메서드(GET, POST, PUT, DELETE 등)를 지원하며, 쿠키, HTTP 헤더, 인증 등 다양한 기능을 제공합니다. 또한, HTTPS와 SSL 인증서를 지원하며, 파일 업로드 및 다운로드 기능도 제공합니다.
        
        requests 모듈은 다른 모듈에 비해 문서화가 잘 되어 있고 사용하기 쉽기 때문에, Python으로 HTTP 요청을 보내는 작업이 필요할 때 많이 사용되는 모듈 중 하나입니다. 예를 들어, 다음과 같이 requests 모듈을 사용하여 간단한 GET 요청을 보낼 수 있음.
        
        ```
        pythonCopy code
        import requests
        
        response = requests.get('https://www.example.com')
        print(response.text)
        
        ```
        
        위 코드는 requests 모듈을 사용하여 '**[https://www.example.com](https://www.example.com/)**' 주소로 GET 요청을 보내고, 응답으로 받은 내용을 출력하는 예시입니다.
        
    - json 모듈
        
        Python의 json 모듈은 JSON(JavaScript Object Notation) 데이터를 파싱하고 생성하기 위한 모듈입니다. JSON은 텍스트 기반의 경량 데이터 교환 형식으로, 데이터를 표현하기 위한 간단한 문법을 제공합니다. JSON은 자바스크립트에서 객체를 표현하는 방식을 확장한 것으로, 다양한 프로그래밍 언어에서 지원하고 있습니다.
        
        json 모듈은 JSON 데이터를 파싱하여 Python 객체로 변환하거나, Python 객체를 JSON 데이터로 변환하는 기능을 제공합니다. 이를 통해 Python 프로그램에서 JSON 데이터를 다룰 수 있습니다.
        
        json 모듈을 사용하면 다음과 같은 작업을 수행할 수 있습니다.
        
        - JSON 데이터를 파싱하여 Python 객체로 변환하기
        - Python 객체를 JSON 데이터로 변환하기
        - 파일로부터 JSON 데이터를 읽어들이거나, 파일에 JSON 데이터를 쓰기
        
        아래는 간단한 예시 코드입니다.
        
        ```python
        pythonCopy code
        import json
        
        # JSON 데이터를 파싱하여 Python 객체로 변환하기
        json_data = '{"name": "John Smith", "age": 30}'
        python_obj = json.loads(json_data)
        print(python_obj)
        
        # Python 객체를 JSON 데이터로 변환하기
        python_obj = {'name': 'John Smith', 'age': 30}
        json_data = json.dumps(python_obj)
        print(json_data)
        
        # 파일로부터 JSON 데이터를 읽어들이기
        with open('data.json', 'r') as f:
            json_data = json.load(f)
            print(json_data)
        
        # 파일에 JSON 데이터를 쓰기
        python_obj = {'name': 'John Smith', 'age': 30}
        with open('data.json', 'w') as f:
            json.dump(python_obj, f)
        
        ```
        
        위 코드에서 json.loads() 함수는 JSON 문자열을 Python 객체로 변환하고, json.dumps() 함수는 Python 객체를 JSON 문자열로 변환합니다. 또한, json.load() 함수와 json.dump() 함수는 파일로부터 JSON 데이터를 읽어들이고, 파일에 JSON 데이터를 쓰는 기능을 제공합니다.
        
    - transformers 모듈
        
        Transformers 모듈은 딥 러닝 모델의 학습, 생성 및 평가를 위한 파이썬 라이브러리입니다. 이 모듈은 자연어 처리 분야에서 최첨단 기술 중 하나인 transformer 모델 아키텍처를 기반으로 합니다. Transformer 모델은 이전의 RNN, LSTM, GRU 등의 기존 모델들보다 높은 성능을 보이면서도 학습 속도가 빠르고, 병렬 처리가 용이하다는 장점이 있습니다.
        
        Transformers 모듈은 다양한 사전 학습 모델(pre-trained model)을 제공합니다. 이러한 사전 학습 모델은 BERT, GPT-2, RoBERTa, T5 등 다양한 종류가 있으며, 각 모델은 특정한 자연어 처리 작업에 최적화된 모델입니다. Transformers 모듈은 이러한 사전 학습 모델을 쉽게 불러와서 다양한 자연어 처리 작업에 활용할 수 있도록 지원합니다.
        
        Transformers 모듈은 PyTorch와 TensorFlow 2.0 라이브러리에서 모두 사용할 수 있습니다. 이 모듈은 다양한 자연어 처리 작업을 위한 함수와 클래스를 제공하며, 이를 사용하여 문장 분류, 문장 생성, 기계 번역, 질의 응답 등 다양한 자연어 처리 작업을 수행할 수 있습니다.
        
    - WandbLogger
        
        **`from pytorch_lightning.loggers import WandbLogger`**는 PyTorch Lightning 라이브러리에서 제공하는 **`WandbLogger`** 클래스를 임포트하는 코드입니다. 이 클래스는 Weights & Biases (W&B) 라이브러리와 PyTorch Lightning을 함께 사용할 때 모델 학습 과정의 로깅을 위한 인터페이스를 제공합니다.
        
        Weights & Biases (W&B)는 모델 학습 과정에서 발생하는 로그 및 지표를 수집, 시각화, 추적하는 데 사용되는 온라인 서비스입니다. PyTorch Lightning은 다양한 로깅 인터페이스를 지원하며, 이 중 하나가 바로 **`WandbLogger`**입니다. 따라서 PyTorch Lightning과 함께 사용하면 모델 학습 과정에서 발생하는 다양한 정보를 W&B 대시보드에서 확인할 수 있습니다. **`WandbLogger`**를 사용하기 위해서는 Weights & Biases 계정이 필요하며, 또한 W&B 라이브러리를 설치하고 초기화해야 합니다.
